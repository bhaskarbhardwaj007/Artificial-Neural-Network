{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9LIaZ0Lt0ad"
      },
      "source": [
        "## Artificial Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZPkK_xyt0ag"
      },
      "source": [
        "# Import Librairies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kc0rmeq31Gq"
      },
      "source": [
        "### 1) Download the dataset from this link https://www.kaggle.com/mohansacharya/graduate-admissions and put the csv file in the same directory as the current notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uuIMWYJt0ak",
        "outputId": "72288054-0a39-480b-d2fd-e4dfea66d6d9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import io\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded['Admission_Predict_Ver1.1.csv']))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-92dd6e35-fffe-4311-90d3-e662df17b508\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-92dd6e35-fffe-4311-90d3-e662df17b508\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Admission_Predict_Ver1.1.csv to Admission_Predict_Ver1.1.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1889sUG35O9"
      },
      "source": [
        "### 2) Analyze the data\n",
        "Print the first five rows to get the idea about the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gliFsf1Yt0ao",
        "outputId": "2ef426c3-bd64-4be0-ac77-d789b3fbde92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Serial No.</th>\n",
              "      <th>GRE Score</th>\n",
              "      <th>TOEFL Score</th>\n",
              "      <th>University Rating</th>\n",
              "      <th>SOP</th>\n",
              "      <th>LOR</th>\n",
              "      <th>CGPA</th>\n",
              "      <th>Research</th>\n",
              "      <th>Chance of Admit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>337</td>\n",
              "      <td>118</td>\n",
              "      <td>4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>9.65</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>324</td>\n",
              "      <td>107</td>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8.87</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>316</td>\n",
              "      <td>104</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>8.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>322</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>8.67</td>\n",
              "      <td>1</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>314</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.21</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Serial No.  GRE Score  TOEFL Score  ...  CGPA  Research  Chance of Admit \n",
              "0           1        337          118  ...  9.65         1              0.92\n",
              "1           2        324          107  ...  8.87         1              0.76\n",
              "2           3        316          104  ...  8.00         1              0.72\n",
              "3           4        322          110  ...  8.67         1              0.80\n",
              "4           5        314          103  ...  8.21         0              0.65\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHr-MMj64CS0"
      },
      "source": [
        "###3) Building X and y data\n",
        "Let us consider the value of `X` contains the following columns of the dataset: [\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR \", \"CGPA\", \"Research\"] and `y` contains the output composed of one column [\"Chance of Admit \"].\n",
        "<b>Build X and y.</b><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBQ_QLQt0aq"
      },
      "source": [
        "X = dataset[[\"GRE Score\", \"TOEFL Score\", \"University Rating\", \"SOP\", \"LOR \", \"CGPA\", \"Research\"]]\n",
        "y = dataset[[\"Chance of Admit \"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL66_scW4LYn"
      },
      "source": [
        "To be used by Numpy we have  to transform X and y by using \".values\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asNgvIBVt0at"
      },
      "source": [
        "X = X.values\n",
        "y = y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v0RxhAq4RBX"
      },
      "source": [
        "### 4) Test-train split\n",
        "Split the data into training and testing datas, the training data is used to train the model, and the testing data is used to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az5y3f-It0aw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K0qXdDpdMl-",
        "outputId": "ff1a4cd0-f119-4202-cfdc-27095ffcbdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"X_train\",X_train)\n",
        "print(\"X_test\",X_test)\n",
        "print(\"y_train\",y_train.T)\n",
        "print(\"y_test\",y_test.T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train [[321.   111.     3.   ...   4.     8.83   1.  ]\n",
            " [316.   111.     4.   ...   5.     8.54   0.  ]\n",
            " [303.   102.     3.   ...   3.     8.5    0.  ]\n",
            " ...\n",
            " [302.    99.     1.   ...   2.     7.25   0.  ]\n",
            " [309.   105.     2.   ...   4.     7.68   0.  ]\n",
            " [314.   106.     2.   ...   3.5    8.25   0.  ]]\n",
            "X_test [[334.   116.     4.     4.     3.5    9.54   1.  ]\n",
            " [314.   108.     4.     4.5    4.     9.04   1.  ]\n",
            " [315.   105.     2.     2.     2.5    7.65   0.  ]\n",
            " [312.   109.     3.     3.     3.     8.69   0.  ]\n",
            " [326.   112.     3.     3.5    3.     9.05   1.  ]\n",
            " [329.   111.     4.     4.5    4.     9.23   1.  ]\n",
            " [290.   100.     1.     1.5    2.     7.56   0.  ]\n",
            " [301.   106.     4.     2.5    3.     8.47   0.  ]\n",
            " [318.   109.     3.     3.5    4.     9.22   1.  ]\n",
            " [320.   112.     4.     3.     4.5    8.86   1.  ]\n",
            " [323.   108.     3.     3.5    3.     8.6    0.  ]\n",
            " [316.   109.     3.     3.5    3.     8.76   0.  ]\n",
            " [322.   103.     4.     3.     2.5    8.02   1.  ]\n",
            " [340.   115.     5.     4.5    4.5    9.45   1.  ]\n",
            " [324.   110.     3.     3.5    3.     9.22   1.  ]\n",
            " [296.    97.     2.     1.5    2.     7.8    0.  ]\n",
            " [332.   108.     5.     4.5    4.     9.02   1.  ]\n",
            " [300.    97.     2.     3.     3.     8.1    1.  ]\n",
            " [298.   101.     2.     1.5    2.     7.86   0.  ]\n",
            " [297.   101.     3.     2.     4.     7.67   1.  ]\n",
            " [307.   105.     2.     2.5    4.5    8.12   1.  ]\n",
            " [297.    99.     4.     3.     3.5    7.81   0.  ]\n",
            " [311.   102.     3.     4.5    4.     8.64   1.  ]\n",
            " [327.   109.     3.     3.5    4.     8.77   1.  ]\n",
            " [327.   112.     3.     3.     3.     8.72   1.  ]\n",
            " [301.   102.     3.     2.5    2.     8.13   1.  ]\n",
            " [330.   120.     5.     4.5    5.     9.56   1.  ]\n",
            " [328.   110.     4.     5.     4.     9.14   1.  ]\n",
            " [312.   105.     2.     2.5    3.     8.12   0.  ]\n",
            " [312.   107.     4.     4.5    4.     8.65   1.  ]\n",
            " [300.   101.     3.     3.5    2.5    7.88   0.  ]\n",
            " [319.   110.     3.     3.     2.5    8.79   0.  ]\n",
            " [305.   104.     2.     2.5    1.5    7.79   0.  ]\n",
            " [323.   113.     4.     4.     4.5    9.23   1.  ]\n",
            " [316.   104.     3.     3.     3.5    8.     1.  ]\n",
            " [319.   108.     3.     3.     3.5    8.54   1.  ]\n",
            " [300.    98.     1.     2.     2.5    8.02   0.  ]\n",
            " [332.   118.     5.     5.     5.     9.64   1.  ]\n",
            " [301.   104.     3.     3.5    4.     8.12   1.  ]\n",
            " [315.   107.     2.     4.     3.     8.5    1.  ]\n",
            " [340.   113.     4.     5.     5.     9.74   1.  ]\n",
            " [301.    97.     2.     3.     3.     7.88   1.  ]\n",
            " [311.   101.     2.     2.5    3.5    8.34   1.  ]\n",
            " [327.   113.     4.     4.5    4.5    9.11   1.  ]\n",
            " [340.   114.     5.     4.     4.     9.6    1.  ]\n",
            " [301.    99.     2.     3.     2.     8.22   0.  ]\n",
            " [337.   118.     4.     4.5    4.5    9.65   1.  ]\n",
            " [327.   111.     4.     4.     4.5    9.     1.  ]\n",
            " [327.   106.     4.     4.     4.5    8.75   1.  ]\n",
            " [328.   116.     5.     5.     5.     9.5    1.  ]\n",
            " [321.   111.     5.     5.     5.     9.45   1.  ]\n",
            " [299.   100.     2.     3.     3.5    7.88   0.  ]\n",
            " [303.   105.     5.     5.     4.5    8.65   0.  ]\n",
            " [301.    96.     1.     3.     4.     7.56   0.  ]\n",
            " [336.   118.     5.     4.5    5.     9.53   1.  ]\n",
            " [320.   103.     3.     3.     3.     7.7    0.  ]\n",
            " [339.   116.     4.     4.     3.5    9.8    1.  ]\n",
            " [318.   110.     3.     4.     3.     8.8    0.  ]\n",
            " [298.   105.     3.     3.5    4.     8.54   0.  ]\n",
            " [296.    95.     2.     3.     2.     7.54   1.  ]\n",
            " [301.    99.     3.     2.5    2.     8.45   1.  ]\n",
            " [308.   103.     2.     2.5    4.     8.36   1.  ]\n",
            " [305.   102.     2.     2.     2.5    8.18   0.  ]\n",
            " [304.   103.     5.     5.     3.     7.92   0.  ]\n",
            " [294.    93.     1.     1.5    2.     7.36   0.  ]\n",
            " [307.   108.     2.     4.     3.5    7.7    0.  ]\n",
            " [324.   113.     4.     4.5    4.5    9.25   1.  ]\n",
            " [329.   114.     5.     4.     5.     9.3    1.  ]\n",
            " [310.   104.     3.     2.     3.5    8.37   0.  ]\n",
            " [318.   106.     3.     2.     3.     8.65   0.  ]\n",
            " [303.   100.     2.     3.     3.5    8.06   1.  ]\n",
            " [326.   110.     3.     3.5    3.5    8.76   1.  ]\n",
            " [320.   101.     2.     2.5    3.     8.62   0.  ]\n",
            " [307.   105.     2.     2.5    3.     7.65   0.  ]\n",
            " [300.    99.     1.     1.     2.5    8.01   0.  ]\n",
            " [313.   106.     2.     2.5    2.     8.43   0.  ]\n",
            " [327.   113.     4.     4.5    5.     9.14   0.  ]\n",
            " [328.   115.     4.     4.5    4.     9.16   1.  ]\n",
            " [305.   102.     2.     1.5    2.5    7.64   0.  ]\n",
            " [305.    96.     4.     3.     4.5    8.26   0.  ]\n",
            " [326.   108.     3.     3.     3.5    8.89   0.  ]\n",
            " [320.   110.     5.     5.     4.5    9.22   1.  ]\n",
            " [309.   105.     4.     3.5    2.     8.18   0.  ]\n",
            " [322.   110.     4.     4.     5.     9.13   1.  ]\n",
            " [323.   104.     3.     4.     4.     8.44   1.  ]\n",
            " [318.   106.     2.     4.     4.     7.92   1.  ]\n",
            " [300.   104.     3.     3.5    3.     8.16   0.  ]\n",
            " [321.   110.     4.     3.5    4.     8.35   1.  ]\n",
            " [311.   107.     4.     4.5    4.5    9.     1.  ]\n",
            " [308.   103.     2.     3.     3.5    8.49   0.  ]\n",
            " [319.   105.     3.     3.     3.5    8.67   1.  ]\n",
            " [327.   116.     4.     4.     4.5    9.48   1.  ]\n",
            " [338.   115.     5.     4.5    5.     9.23   1.  ]\n",
            " [315.   104.     3.     3.     2.5    8.33   0.  ]\n",
            " [320.   112.     2.     3.5    3.5    8.78   1.  ]\n",
            " [299.    94.     1.     1.     1.     7.34   0.  ]\n",
            " [315.   106.     3.     4.5    3.5    8.42   0.  ]\n",
            " [329.   114.     2.     2.     4.     8.56   1.  ]\n",
            " [318.   110.     1.     2.5    3.5    8.54   1.  ]\n",
            " [314.   105.     3.     3.5    2.5    8.3    0.  ]]\n",
            "y_train [[0.77 0.71 0.62 0.72 0.75 0.64 0.66 0.82 0.73 0.71 0.71 0.94 0.83 0.94\n",
            "  0.45 0.8  0.71 0.86 0.45 0.53 0.76 0.9  0.75 0.79 0.74 0.64 0.52 0.97\n",
            "  0.85 0.43 0.74 0.72 0.87 0.54 0.84 0.46 0.86 0.97 0.65 0.71 0.61 0.64\n",
            "  0.79 0.36 0.93 0.72 0.95 0.76 0.71 0.71 0.9  0.56 0.88 0.66 0.62 0.68\n",
            "  0.8  0.84 0.52 0.48 0.61 0.79 0.69 0.54 0.61 0.64 0.76 0.68 0.73 0.93\n",
            "  0.58 0.65 0.65 0.47 0.59 0.85 0.79 0.71 0.8  0.67 0.64 0.78 0.62 0.78\n",
            "  0.71 0.9  0.86 0.57 0.79 0.64 0.62 0.38 0.63 0.49 0.68 0.8  0.81 0.34\n",
            "  0.86 0.57 0.78 0.92 0.81 0.78 0.68 0.72 0.71 0.74 0.71 0.7  0.74 0.71\n",
            "  0.76 0.71 0.79 0.59 0.95 0.61 0.63 0.91 0.82 0.58 0.64 0.73 0.78 0.77\n",
            "  0.63 0.66 0.92 0.42 0.69 0.74 0.58 0.75 0.85 0.47 0.81 0.61 0.93 0.82\n",
            "  0.77 0.72 0.79 0.52 0.78 0.57 0.96 0.52 0.66 0.64 0.86 0.76 0.67 0.7\n",
            "  0.86 0.65 0.75 0.84 0.83 0.84 0.89 0.61 0.51 0.48 0.97 0.59 0.92 0.86\n",
            "  0.58 0.57 0.75 0.97 0.72 0.74 0.9  0.73 0.46 0.82 0.94 0.64 0.65 0.71\n",
            "  0.6  0.74 0.92 0.42 0.79 0.92 0.34 0.56 0.73 0.61 0.84 0.75 0.62 0.73\n",
            "  0.8  0.86 0.79 0.78 0.73 0.64 0.84 0.81 0.76 0.73 0.77 0.69 0.66 0.94\n",
            "  0.62 0.67 0.78 0.91 0.78 0.93 0.56 0.91 0.72 0.81 0.79 0.93 0.91 0.77\n",
            "  0.55 0.8  0.76 0.7  0.84 0.62 0.52 0.78 0.88 0.46 0.68 0.92 0.67 0.58\n",
            "  0.75 0.42 0.56 0.6  0.73 0.56 0.72 0.49 0.73 0.77 0.63 0.81 0.85 0.59\n",
            "  0.65 0.76 0.8  0.71 0.63 0.95 0.96 0.89 0.9  0.56 0.93 0.47 0.89 0.91\n",
            "  0.73 0.69 0.73 0.93 0.44 0.91 0.73 0.87 0.82 0.67 0.92 0.8  0.54 0.82\n",
            "  0.84 0.62 0.5  0.63 0.57 0.52 0.62 0.61 0.7  0.46 0.71 0.81 0.91 0.91\n",
            "  0.67 0.64 0.74 0.94 0.85 0.66 0.95 0.68 0.37 0.7  0.76 0.68 0.73 0.74\n",
            "  0.77 0.72 0.53 0.87 0.85 0.66 0.9  0.7  0.9  0.69 0.72 0.76 0.76 0.79\n",
            "  0.83 0.78 0.58 0.5  0.57 0.94 0.7  0.71 0.74 0.38 0.81 0.67 0.78 0.49\n",
            "  0.75 0.65 0.62 0.65 0.64 0.53 0.52 0.65 0.8  0.53 0.89 0.79 0.8  0.8\n",
            "  0.76 0.7  0.69 0.76 0.51 0.87 0.91 0.88 0.96 0.93 0.65 0.36 0.82 0.47\n",
            "  0.88 0.71 0.7  0.67 0.89 0.57 0.89 0.87 0.87 0.96 0.64 0.49 0.59 0.78\n",
            "  0.69 0.79 0.96 0.94 0.81 0.79 0.95 0.66 0.69 0.8  0.94 0.71 0.94 0.84\n",
            "  0.64 0.93 0.96 0.87 0.72 0.57 0.55 0.62]]\n",
            "y_test [[0.93 0.84 0.39 0.77 0.74 0.89 0.47 0.57 0.68 0.82 0.45 0.77 0.61 0.94\n",
            "  0.89 0.49 0.87 0.65 0.54 0.57 0.67 0.54 0.68 0.79 0.74 0.68 0.93 0.82\n",
            "  0.64 0.73 0.59 0.72 0.53 0.89 0.72 0.71 0.61 0.94 0.68 0.56 0.96 0.44\n",
            "  0.7  0.89 0.9  0.64 0.92 0.84 0.76 0.94 0.93 0.68 0.77 0.54 0.94 0.64\n",
            "  0.96 0.63 0.69 0.44 0.68 0.7  0.62 0.71 0.46 0.48 0.89 0.86 0.7  0.71\n",
            "  0.64 0.79 0.7  0.58 0.58 0.62 0.83 0.78 0.59 0.54 0.8  0.92 0.65 0.86\n",
            "  0.73 0.64 0.71 0.72 0.78 0.66 0.73 0.9  0.91 0.67 0.73 0.42 0.72 0.72\n",
            "  0.67 0.54]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oIRCGvYt0az"
      },
      "source": [
        "###  5) Feature scaling\n",
        "####  Feature scaling or Data Normalization is a method used to standardize the range of independent variables or features of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2Tj0y01t0a0"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train,y_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRllrtkot0a3"
      },
      "source": [
        "### 6)Simple Neural Network also known as Perceptron\n",
        "We will construct hidden layer(In this case 2 hidden layers are sufficients). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naX8sLIDt0a4"
      },
      "source": [
        "# Import Librairies\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGexyTYot0a7"
      },
      "source": [
        "# 9-Building a model\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(14, activation= 'relu', input_dim=7),\n",
        "    layers.Dense(14, activation= 'sigmoid'),\n",
        "    tf.keras.layers.Softmax(axis=-1)\n",
        "  ])\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy', 'mse'])\n",
        "  return model\n",
        "\n",
        "\n",
        "model1 = build_model()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYvRTgPYt0a9"
      },
      "source": [
        "### 7) Fit your trained model\n",
        "A model that is well-fitted produces more accurate outcomes, a model that is overfitted matches the train data too closely, to prevent it reduce the model's complexity or increase the size of the dataset and a model that is underfitted doesn’t match the train data closely enough results in high train loss, to prevent it increase the model complexity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1WntMTCt0a-",
        "outputId": "530a5efc-0e0f-41c2-9bb7-058f29a52a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 50\n",
        "epochs = 30\n",
        "fitting = model1.fit(X_train,y_train,validation_data=(X_test,y_test), batch_size=batch_size,epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.4459 - accuracy: 0.0000e+00 - mse: 0.4459 - val_loss: 0.4309 - val_accuracy: 0.0000e+00 - val_mse: 0.4309\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4459 - accuracy: 0.0000e+00 - mse: 0.4459 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.0000e+00 - mse: 0.4458 - val_loss: 0.4308 - val_accuracy: 0.0000e+00 - val_mse: 0.4308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9vIC7HL4uMA"
      },
      "source": [
        "### 8) Make a prediction for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upK4NS34t0bD",
        "outputId": "47da5af4-4f77-4d9c-f0c3-43c19621adea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "x = np.array([220,100,2,3.5,3,8,0],dtype = 'float32')\n",
        "print(x)\n",
        "x = x.reshape(-1, 7)\n",
        "x = scaler.transform(x)\n",
        "model1.predict(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[220.  100.    2.    3.5   3.    8.    0. ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05432913, 0.10702512, 0.05286033, 0.10193793, 0.05933245,\n",
              "        0.06749596, 0.09837451, 0.06431349, 0.06093952, 0.05673836,\n",
              "        0.05795191, 0.06251211, 0.10713977, 0.04904939]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}